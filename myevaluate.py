import os
import numpy as np
import argparse
import clip
import json
import torch
import glob
import torch.nn.parallel
import torch.optim
import torch.utils.data
import sys
sys.path.append('.')
import tensorflow as tf2
tf = tf2.compat.v1
import pycocotools.mask as mask_util

from tensorflow import io
from PIL import Image
from tqdm import tqdm


# parameters
def get_parser():
    '''Parse the config file.'''

    parser = argparse.ArgumentParser(description='Freemasks generated by Openseg and Clip evaluation')
    parser.add_argument('--openseg_model_dir',
                    type=str,
                    default="/home/diqihe/code/openscene-main/openseg_exported_clip",
                    help='openseg model directory')
    parser.add_argument('--img_path',
                    type=str,
                    default="/home/diqihe/code/openscene-main/test/002.jpg",
                    help='image path')
    parser.add_argument('--palette_path',
                    type=str,
                    default="/home/diqihe/code/openscene-main/run/palette.json",
                    help='palette path')
    parser.add_argument('--coco_json_path',
                    type=str,
                    default="/home/diqihe/code/openscene-main/annotations/annotations_trainval2017/annotations/instances_val2017.json",
                    help='coco json path')
    parser.add_argument('--coco_json_saved_path',
                    type=str,
                    default="/home/diqihe/code/openscene-main/freemasks_results/freemasks.json",
                    help='coco json saved path')
    parser.add_argument('--mask_dir',
                    type=str,
                    default="/home/diqihe/code/openscene-main/freemasks_results",
                    help='mask directory')
    parser.add_argument('--categories_names', type=list, default=['book', 'person', 'car', 'telephone kiosk', 'coffee'], #
                                                    help="categories names: list different names. Each category \
                                                    is a string under a "''", different names of a category are in \
                                                    it. All categories consist of a list. E.g. ['lady, ladies, girl, girls', 'book'] \
                                                    creates two categories of 'lady or ladies or girl or girls' and 'book'.")

    args = parser.parse_args()
    return args


# load openseg model
def load_openseg_model():

    saved_model_dir = args.openseg_model_dir    #@param {type:"string"}
    openseg_model = tf2.saved_model.load(saved_model_dir, tags=[tf.saved_model.tag_constants.SERVING],)

    return openseg_model


# load clip model
def load_clip_model(model_name="ViT-L/14@336px"):
    # "ViT-L/14@336px" # the big model that OpenSeg uses

    print("Loading CLIP {} model...".format(model_name))
    clip_pretrained, _ = clip.load(model_name, device='cuda', jit=False)
    print("Finish loading")
    return clip_pretrained


# Openscene: build text features using clip
def extract_clip_feature(categories_names, clip_pretrained):
    
    if isinstance(categories_names, str):
        lines = categories_names.split(',')
    elif isinstance(categories_names, list):
        lines = categories_names
    else:
        raise NotImplementedError

    labels = []
    for line in lines:
        label = line
        labels.append(label)
    text = clip.tokenize(labels)
    text = text.cuda()
    text_features = clip_pretrained.encode_text(text)

    return text_features


# Read bytes for OpenSeg model running
def read_bytes(path):
    
    with io.gfile.GFile(path, 'rb') as f:
        file_bytes = f.read()
        
    return file_bytes


# extract openseg image feature
def extract_openseg_img_feature(img_path, openseg_model, text_emb, img_size=None, regional_pool=True):
    '''Extract per-pixel OpenSeg features.'''

    # load RGB image
    np_image_string = read_bytes(img_path)

    # run OpenSeg
    results = openseg_model.signatures['serving_default'](
            inp_image_bytes=tf.convert_to_tensor(np_image_string),
            inp_text_emb=text_emb)
    img_info = results['image_info']
    crop_sz = [
        int(img_info[0, 0] * img_info[2, 0]),
        int(img_info[0, 1] * img_info[2, 1])
    ]
    if regional_pool:
        image_embedding_feat = results['ppixel_ave_feat'][:, :crop_sz[0], :crop_sz[1]]
    else:
        image_embedding_feat = results['image_embedding_feat'][:, :crop_sz[0], :crop_sz[1]]
    if img_size is not None:
        feat_2d = tf.cast(tf.image.resize_nearest_neighbor(
            image_embedding_feat, img_size, align_corners=True)[0], dtype=tf.float16).numpy()
    else:
        feat_2d = tf.cast(image_embedding_feat[[0]], dtype=tf.float16).numpy()

    feat_2d = torch.from_numpy(feat_2d).permute(2, 0, 1)

    return feat_2d


# evaluate
def main():
    '''Main function.'''
    
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("using {} device.".format(device))

    torch.backends.cudnn.enabled = False

    # short hands
    openseg_model = load_openseg_model()
    clip_model = load_clip_model()
    args.feat_dim = 768 # CLIP feature dimension
    args.text_emb = tf.zeros([1, 1, args.feat_dim])
    text_emb = args.text_emb

    # palette
    palette_path = args.palette_path
    with open(palette_path, "rb") as f:
        pallette_dict = json.load(f)
        pallette = []
        for v in pallette_dict.values():
            pallette += v

    # json
    ann_dict = {}
    ann_dict_ = json.load(open(args.coco_json_path))
    ann_dict['categories'] = ann_dict_['categories']
    images_list = []
    annotations_list = []
    boxes = []
    annotation_id = 1

    # all categories list 
    categories_list = []
    for dict in ann_dict['categories']:
        # category_names = dict['supercategory'] + ', ' + dict['name']
        category_names = dict['name']
        categories_list.append(category_names)
    categories_names = categories_list

    # get all image paths
    all_img_paths = glob.glob("/home/diqihe/code/openscene-main/test/*.jpg")
    # all_img_paths = [args.img_path]

    # iteration
    with torch.no_grad():

        for path in tqdm(all_img_paths):

            img_path = path
            # get image information
            img = Image.open(img_path)
            width, height = img.size

            # get text features
            text_features = extract_clip_feature(categories_names, clip_model)
            # normalize dim = C(768)
            text_features= text_features / text_features.norm(dim=-1, keepdim=True)
            # transpose [C, 768] -> [768, C]
            text_features = text_features.t()

            # extract openseg features
            feat_2d = extract_openseg_img_feature(img_path, openseg_model, text_emb, img_size=[240, 320]).to(device)
            # normalize dim = C(768)
            feat_2d = feat_2d / (feat_2d.norm(dim=0, keepdim=True) + 1e-5)
            # transpose CHW - HCW - HWC
            feat_2d = feat_2d.transpose(0, 1).transpose(1, 2)
            feat_2d = (feat_2d).half()
            
            # compute
            attention = feat_2d @ text_features
            # transpose HWC- HCW - CHW   
            attention = attention.transpose(1, 2).transpose(0, 1)
            # normalize dim = C
            attention = attention / (attention.norm(dim=0, keepdim=True) + 1e-5)

            # argmax
            prediction = attention.argmax(0)
            # tensor -> array
            prediction = prediction.cpu().numpy()
            prediction = prediction.astype(np.uint8) + 1 
            # generate image
            coarse_mask = Image.fromarray(prediction)
            # get palette
            coarse_mask.putpalette(pallette)
            # save image
            mask_dir = args.mask_dir
            img_name = img_path.split('/')[-1].split('.')[0]
            coarse_mask.save(os.path.join(mask_dir, img_name) + ".png")

            # keep valid masks
            masks = []
            mask_dict = {}
            for category_id in range(1, len(attention)+1):
                mask = (prediction == category_id).astype(np.uint8)
                if mask.sum((0,1)) > 0:
                    mask_dict['category_id'] = category_id
                    mask_dict['mask'] = mask
                    masks.append(mask_dict.copy())

            # mask to box
            for mask_dict in masks:
                mask = mask_dict['mask']
                ys, xs = np.where(mask)
                box = [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]
                boxes.append(box)

            # coco format
            # images list
            try:
                img_id = int(img_name)
            except:
                img_id = int(img_name.split('_')[-1])
            cur_image_dict = {'file_name': img_path.split('/')[-1],
                                'height': height,
                                'width': width,
                                'id':  img_id}
            images_list.append(cur_image_dict)

            # annotations list
            for mask_dict in masks:
                mask = mask_dict['mask']
                category_id = mask_dict['category_id']
                rle = mask_util.encode(np.array(mask[:, :, None], order="F", dtype="uint8"))[0]
                rle['counts'] = rle['counts'].decode('ascii')
                cur_ann_dict = {'segmentation': rle,
                                'bbox': boxes[annotation_id-1],
                                'iscrowd': 0,
                                'image_id': img_id,
                                'category_id': category_id,
                                'mask_id':  annotation_id}
                annotation_id += 1
                annotations_list.append(cur_ann_dict)
    
    # save json
    ann_dict['images'] = images_list
    ann_dict['annotations'] = annotations_list
    json.dump(ann_dict, open(args.coco_json_saved_path, 'w'))
    #json.dump(annotations_list, open(save_path+'ann', 'w'))
    print("Done: {} images, {} annotations.".format(len(images_list), len(annotations_list)))

    # end
    print('Finish evaluating')


if __name__ == '__main__':
    
    args = get_parser()
    main()